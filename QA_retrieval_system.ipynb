{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_QA_retrieval_system.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvXCAsK2xsT0"
      },
      "source": [
        "\n",
        "**IR Project on Question Answering System**\n",
        "\n",
        "Below Code is for QA retrieval "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fsWnmqNifdu"
      },
      "source": [
        "#necessary installs\n",
        "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers\n",
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H1XeJyYjF9j"
      },
      "source": [
        "#necessary imports\n",
        "import torch\n",
        "import wikipedia as wiki\n",
        "import pprint as pp\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from collections import OrderedDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85LY4VpNIFS6"
      },
      "source": [
        "# to make output more readable,  turning off the token sequence length warning\n",
        "import logging\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gv8rKbcjGL_"
      },
      "source": [
        "class DocumentReader:\n",
        "    def __init__(self, pretrained_model_name_or_path):\n",
        "        self.READER_PATH = pretrained_model_name_or_path\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
        "        self.max_len = self.model.config.max_position_embeddings\n",
        "        self.chunked = False\n",
        "\n",
        "    def tokenize(self, question, text):\n",
        "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        if len(self.input_ids) > self.max_len:\n",
        "            self.inputs = self.chunkify()\n",
        "            self.chunked = True\n",
        "\n",
        "    def chunkify(self):\n",
        "        \"\"\" \n",
        "        Break up a long article into chunks that fit within the max token\n",
        "        requirement for that Transformer model. \n",
        "\n",
        "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
        "        [CLS] question tokens [SEP] context tokens [SEP].\n",
        "        \"\"\"\n",
        "\n",
        "        # create question mask based on token_type_ids\n",
        "        # value is 0 for question tokens, 1 for context tokens\n",
        "        qmask = self.inputs['token_type_ids'].lt(1)\n",
        "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
        "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
        "        # having to add an ending [SEP] token to the end\n",
        "\n",
        "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
        "        chunked_input = OrderedDict()\n",
        "        for k,v in self.inputs.items():\n",
        "            q = torch.masked_select(v, qmask)\n",
        "            c = torch.masked_select(v, ~qmask)\n",
        "            chunks = torch.split(c, chunk_size)\n",
        "            \n",
        "            for i, chunk in enumerate(chunks):\n",
        "                if i not in chunked_input:\n",
        "                    chunked_input[i] = {}\n",
        "\n",
        "                thing = torch.cat((q, chunk))\n",
        "                if i != len(chunks)-1:\n",
        "                    if k == 'input_ids':\n",
        "                        thing = torch.cat((thing, torch.tensor([102])))\n",
        "                    else:\n",
        "                        thing = torch.cat((thing, torch.tensor([1])))\n",
        "\n",
        "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
        "        return chunked_input\n",
        "\n",
        "    def get_answer(self):\n",
        "        if self.chunked:\n",
        "            answer = ''\n",
        "            sum = -100\n",
        "            #print(type(sum))\n",
        "            for k, chunk in self.inputs.items():\n",
        "                answer_start_scores, answer_end_scores = self.model(**chunk, return_dict = False)\n",
        "                answer_start = torch.argmax(answer_start_scores)\n",
        "\n",
        "                #print(answer_start)\n",
        "                answer_end = torch.argmax(answer_end_scores) + 1\n",
        "                #print(type(answer_start))\n",
        "                answer_start_max = answer_start_scores[0][answer_start]\n",
        "                answer_end_max = answer_end_scores[0][answer_end - 1]\n",
        "\n",
        "                temp = answer_start_max + answer_end_max\n",
        "\n",
        "                if (temp > sum):\n",
        "                  sum = temp\n",
        "                  ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
        "                #if ans != '[CLS]':\n",
        "            return ans\n",
        "            \n",
        "        else:\n",
        "            answer_start_scores, answer_end_scores = self.model(**self.inputs, return_dict = False)\n",
        "\n",
        "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
        "        \n",
        "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
        "                                              answer_start:answer_end])\n",
        "\n",
        "    def convert_ids_to_string(self, input_ids):\n",
        "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaGqUGvMjGXd",
        "outputId": "97ccd444-f3f7-4422-be1d-c1bb7edcfd60"
      },
      "source": [
        "#Validation part\n",
        "\n",
        "\n",
        "questions = [\n",
        "             'Who is the Prime Minister of India?',\n",
        "    'Who is the CEO of Google?',\n",
        "    'Who is John Snow from Game of Thrones?'\n",
        "]\n",
        "\n",
        "# if you trained your own model using the training cell earlier, you can access it with this:\n",
        "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
        "reader = DocumentReader(\"bert-large-uncased-whole-word-masking-finetuned-squad\") \n",
        "\n",
        "for question in questions:   \n",
        "    print(f\"Question: {question}\")\n",
        "    results = wiki.search(question)\n",
        "    print(results)\n",
        "    for i in range(2):\n",
        "      page = wiki.page(results[i], auto_suggest=False)\n",
        "      #print(f\"Top wiki result: {page}\")\n",
        "      text = page.content\n",
        "      reader.tokenize(question, text)\n",
        "      print(f\"Answer: {reader.get_answer()}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: Who is the Prime Minister of India?\n",
            "['Deputy Prime Minister of India', 'List of prime ministers of India', 'Prime Minister of India', 'List of prime ministers of India by longevity', 'List of prime ministers of India by previous experience', 'Living prime ministers of India', 'Spouse of the prime minister of India', 'Union Council of Ministers', \"Prime Minister's Office (India)\", 'Minister of Defence (India)']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Answer: prime minister of india\n",
            "Answer: narendra modi\n",
            "\n",
            "Question: Who is the CEO of Google?\n",
            "['Google', 'Alphabet Inc.', 'Susan Wojcicki', 'Sundar Pichai', 'Google Maps', 'Google Meet', 'Larry Page', 'Google Drive', 'Google Sheets', 'Google Lens']\n",
            "Answer: sundar pichai\n",
            "Answer: sundar pichai\n",
            "\n",
            "Question: Who is John Snow from Game of Thrones?\n",
            "['A Game of Thrones', 'List of Game of Thrones characters', 'Game of Thrones (season 6)', 'The Iron Throne (Game of Thrones)', 'Winterfell (Game of Thrones episode)', 'Game of Thrones (season 8)', 'Game of Thrones (season 5)', 'Lord Snow', 'Game of Thrones (season 1)', 'Game of Thrones (season 3)']\n",
            "Answer: illegitimate son of eddard stark\n",
            "Answer: [CLS]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}